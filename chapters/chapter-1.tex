\chapter{Introduction} \label{ch:introduction}
\section{Statistical Physics}
Statistical physics is a field of study with it's roots in thermodynamics. The initial goal was to understand the thermal properties of matter and the behavior of gases \cite{Kardar}. It has been studied since the 1600's with contributions coming from such prominent physicists as Bernoulli, Maxwell, and Boltzmann \cite{Flamm1998}. The initial studies of the topic have revolved around the understanding of heat processes and engines. These are systems that contain moles of particles interacting with each other and their environment, like heat reservoirs. As technology increased, scientists began to test these statistical theories at lower and lower temperatures. Eventually, the discovery of phenomena such as superconductivity and superfluidity led to the necessity for a new theoretical picture in order to describe the underlying physics. The theory of Quantum mechanics explain these phenomena with the help of statistical mechanics. Experimentalists are now at the point where temperatures as low as micro Kelvin are used in experiments \cite{Onofrio2016}\cite{Es2010}. This technology has given rise to the study of atoms in cold and ultra cold environments. These experiments are perfect testing grounds for some of the fundamental quantum mechanical problems that show up in undergraduate and graduate textbooks.

\section{Quantum Mechanical Traps}
To understand these experiements, it's necessary to solve the Schr\"odinger for the harmonic oscillator and infinite potential well. Before continuing, it should be noted that only systems of non-interacting fermions are considered. The energy eigenvalues are calculated in Appendix A with help from Griffiths \cite{Griffiths}. These eigenvalues give the linear spectrum 
\begin{equation}
    E_n=\hbar\omega\qty(n+\frac{1}{2})
\end{equation}
for the simple harmonic oscillator and the quadratic equation 
\begin{equation}
    E_n=\frac{n^2\pi^2\hbar^2}{2ma^2}
\end{equation}
for the infinite potential well, where $a$ is the length of the well. These spectra can be physically realized in experimental systems to create areas with a fixed number of particles that are trapped, or traps. The harmonic oscillator spectrum can be created with a magneto-optical trap as shown by Phillips et. al \cite{Phillips_1998}. These traps are generated by using lasers and magnetic fields to hold a finite number of particles at very low temperatures. The development of this method won the Nobel prize in 1997 for its impact on the physics community \cite{nobelprize.org_1997}. More recently, box traps have been realized by using these magneto-optical traps to put atoms onto an atomic chip \cite{Es2010}. There are other traps that consist of more exotic shapes, like a trap that is a box in one dimension and circular in another \cite{Mukherjee2017}. These more complicated shapes require numerical solutions since they may not be able to be solved analytically. In general, measurements of the atomic gas temperature is done by releasing the atoms and measuring the momentum distribution \cite{Es2010}[OTHER REF?]. These distributions are then fit using statistical mechanics to determine thermodynamic features of the system. 

\section{Ensemble Theory}
Since it is impossible to measure what one single particle is doing in a gas, statistical mechanics uses ensembles to calculate the properties of the system. An ensemble is a grouping of microstates that correspond to a mixed macrostate \cite{Kardar}. A microstate is a particular distribution of particles in the system. For example, if a box has two coins in it, shaking the box can only produce four states, $(H,H),(H,T),(T,H),(T,T)$ \cite{Blundell}. These are the microstates of the system. A macrostate is the collection of all systems that give a specific equilibrium condition. Going back to the coin example, there are three macrostates, both heads, both tails, and a head and a tail. The last macrostate consists of the two microstates $(H,T)$ and $(T,H)$ since they each contain a head and a tail. These macrostates can then be analyzed in as an ensemble. There are three ensembles that are used in statistical mechanics, the microcanonical, canonical, and grand canonical ensembles. The microcanonical ensemble describes a system that is mechanically and adiabatically isolated \cite{Kardar}. This means the system is disconnected from any sort of heat or work that could be applied or drawn from the system. Since there's no input or output, the internal energy of the system is fixed and as a result, the equilibrium temperature is also fixed. 
The canonical ensemble describes a system that allows heat and work to be an input in the system. This ensemble is usually modelled as a system that's in contact with a reservoir that's large enough so that interactions between the two do not effect the reservoir's temperature. 
The grand canonical ensemble is similar to the canonical ensemble except now it allows chemical work to be done. This means that the system can have a varying number of particles coming in and out. This ensemble is very useful when trying to describe something like heating a classroom. In this system, the number of particles is allowed to vary, like particles leaving and entering through the door of the classroom. In general, the average number of particles is accounted for in the chemical work term, $\mu$ which is known as the chemical potential. This term will set the average number of particles in the system. 
When working with the canonical and grand canonical ensembles, it is advantageous to define a quantity known as the partition function. The partition function is a function of thermodynamic state variables that is defined as a sum over all the state of Boltzmann factors \cite{Blundell}. In other words, it is a Boltzmann distributions that are related to the thermodynamic variables such as temperature. This function is used to derive thermodynamical properties of the system, like specific heat and internal energy.
The canonical partition function is written as 
\begin{equation}
    Z_C=\sum_i e^{-\beta E_i}
\end{equation}
where $E_i$ is the energy at level $i$ and $\beta$ is the inverse temperature. From this partition function, the probability of a state with energy level $E_i$, being occupied is written as  
\begin{equation}
    \avg{p_i}_C\equiv \avg{n_i}=\frac{e^{-\beta E_i}}{Z_C}.
\end{equation}
Since the canonical ensemble is a closed system with a set number of particles, the sum of all probabilities, $\sum_i p_i$, equals the number of particles in the system. Turning to the grand canonical ensemble, only the average number of particles in the system is known. Since this is controlled by the chemical potential $\mu$, the partition function is written as
\begin{gather}
    Z_{GC}=\sum_i e^{-\beta(E_i-\mu N_i)}
\end{gather}
where $N_i$ is the number of particles. Again, the occupation probability can be written from the partition function as 
\begin{equation}
    \avg{p_i}_{GC}\equiv\avg{p_i}=\frac{e^{-\beta(E_i-\mu)}}{Z_{GC}}.
\end{equation}
When considering the case of non-interacting fermions, there is another formulation of the occupation probability and it's connection to the grand canonical partition function. This is worked out in Appendix B with the main results being 
\begin{gather}
    Z_{GC}=\prod_i \frac{1}{1-p_i}=\prod_i \frac{1}{q_i}\\
    \avg{p_i}=\frac{1}{1+e^{\beta(E_i-\mu)}}.
\end{gather}
Comparing the two partition functions, the difference between ensembles is the inclusion of the chemical potential in the grand canonical formulation. The partition functions can be related to one another as
\begin{equation}
    Z_{GC}=\sum_i e^{-\beta(E_i-\mu N_i)}=\sum_i e^{\beta \mu N_i} Z_{C}.
\end{equation}
From this equation, it is easy to assume that switching between the two ensembles only requires the additional $e^{\beta\mu}$ term in the canonical ensemble. 

In the thermodynamic limit, that is, when the temperature and the number of particles are large enough, the these two ensembles give nearly the same result, so it is correct to use either. This can be shown by looking at the Boltzmann probabilities in each partition function. For very high temperature, $\beta \sim \frac{1}{T}$ is very small. While there is a difference in the chemical potential, in general 
\begin{equation}
    e^{-\beta E_i}=e^{-\beta(E_i-\mu N_i)}
\end{equation}
for $\beta\ll 1$. Typically, the grand canonical ensemble is used since it is easier to calculate Equation (1.8) rather than Equation (1.4). Along with this, the restriction to the exact number of particles in the canonical ensemble makes the summation for the partition function difficult to compute. This restriction doesn't exist in the grand canonical ensemble since only the average number of particles is considered. The problem that arises from working in the grand canonical ensemble is when working away from the thermodynamic limit, like in the case of the previously mentioned cold atom experiments. In these experiments, the momentum distribution is measured to determine the occupation probability and energy spectrum \cite{Es2010}[REF?]. The number of particles is fixed and the temperature is very low, which puts the system out of the thermodynamic limit. Since the grand canonical calculations are easier, one may naively use this ensemble to attempt to find the temperature. Unfortunately, this will result in an error in such measurements. To understand this error, it is appropriate to look at a simple example. 

\section{Simple Example}
To quantify the error between the two ensembles, the following simple case is provided. Specifically, the inverse temperature $\beta$ will be calculated in each of the ensembles to determine the difference in the measurement. Consider a system with two energy levels, $\epsilon_1=0$ and $\epsilon_2=\Delta$ and one fermion. In the canonical ensemble, the occupation probability of a single level is given by the following equation.
\begin{equation}
    \avg{n_i}=\frac{e^{-\beta\epsilon_i}}{\sum_i e^{-\beta\epsilon_i}}
\end{equation}
For the case of two energy levels, the probabilities are the following.
\begin{gather}
    \avg{n_1}=\frac{1}{1+e^{-\beta\Delta}}\\
    \avg{n_2}=\frac{e^{-\beta\Delta}}{1+e^{-\beta\Delta}}.
\end{gather}
In the grand canonical ensemble, the occupation probability of each energy level is given by 
\begin{equation}
    \avg{p_i}=\frac{1}{1+e^{\beta(\epsilon_i-\mu \avg{N})}}=\frac{e^{-\beta(\epsilon_i-\mu \avg{N})}}{1+e^{-\beta(\epsilon_i-\mu \avg{N})}}
\end{equation}
where $\avg{N}$ is the average number of particles in the system. In this case, there's only one particle in the system, so $\avg{N}=1$. The probabilities for this system are the following.
\begin{gather}
    \avg{p_1}_{GC}=\frac{e^{\beta\mu}}{1+e^{\beta\mu}}=\frac{1}{1+e^{-\beta\mu}}\\
    \avg{p_2}_{GC}=\frac{e^{-\beta(\Delta-\mu)}}{1+ e^{-\beta(\Delta-\mu)}}
\end{gather}
Here, the chemical potential, $\mu$, is used to control the average number of particles in the system. There is one particle so the probabilities of level one and level two should add up to one. For this case, the chemical potential is found as 
\begin{gather}
    \avg{p_1}+\avg{p_2}=\avg{N}\nonumber\\
    \frac{1}{1+e^{-\beta\mu}}+\frac{e^{-\beta(\Delta-\mu)}}{1+e^{-\beta(\Delta-\mu)}}=1\nonumber\\
    \mu=\frac{\Delta}{2}.
\end{gather}
With the value of $\mu$ determined, the occupation probability for each level is
\begin{gather}
    \avg{p_1}=\frac{1}{1+e^{-\frac{\beta\Delta}{2}}}\\
    \avg{p_2}=\frac{e^{-\frac{\beta\Delta}{2}}}{1+ e^{-\frac{\beta\Delta}{2}}}/
\end{gather}
To see the difference in the measured $\beta$ value between the two ensembles, the occupation probabilities for the same level can be set equal to each other. The canonical ensemble temperature will be denoted as $\beta$ while the grand canonical ensemble temperature will be denoted as $\beta^*$. Equating the two probabilities, 
\begin{gather}
    \avg{n_1}=\frac{1}{1+e^{-\beta\Delta}}=\frac{1}{1+e^{-\frac{\beta^*\Delta}{2}}}=\avg{p_1}.
\end{gather}
It can be observed that there is only equality between the two sides when $\beta^*=2\beta$. This shows a one hundred percent difference in the value of the measured temperature between the two ensembles. The error lies in the use of the chemical potential in the grand canonical ensemble. The chemical potential only sets the average number of particles in the system. This means that there is some fluctuation of particle number when using the grand canonical ensemble. Meanwhile, the canonical ensemble sets the number of particles exactly and therefore perfectly describes the case of cold atoms trapped in a potential trap. It turns out that this problem has been previously studied to produce analytical solutions for certain cases \cite{Borr1993} \cite{Schon1996} \cite{Hatem2020}. 

\section{Recursive relations and exact solutions}
To solve the problem given in the previous section, previous physicists have created recursive equations that connect the canonical ensemble to the grand canonical ensemble is recursive \cite{Borr1993} \cite{Schon1996} \cite{Hatem2020}. Implementing a recursive equation allows one to build the partition up to a point and stop further calculations. The chosen stopping point is the number of particles that in the system since any partition function after that point does not describe the particluar state. Borrmann introduced a method to do this by summing over all the previous states \cite{Borr1993}. The steps for this recursion begin with the starting point at $Z(0)=1$ and defining 
\begin{equation}
    S(k):=\sum_j \exp{-\beta k \epsilon(j)}
\end{equation}
where the sum over $j$ is summing over all possible states. In the case of fermions, the partition function is written as
\begin{equation}
    Z_F(N)=\frac{1}{N_!}\sum_{r_1}\sum_{r_2\neq r_1}...\sum_{r_N\neq r_1,r_2...r_{N-1}} \exp{-\beta \sum_{k=1}^N \epsilon(r_k)}
\end{equation}
with $k$ denoting the particle and $r_k$ denoting the state that particle $k$ is in. The $\epsilon(r_k)$ term is then the energy of particle $k$ as state $r_k$. The sum can be split up into two terms
\begin{align}
    Z_F(N)&=\frac{1}{N}\qty[\frac{1}{(N-1)!} \sum_{r_1}\sum_{r_2\neq r_1}...\sum_{r_{N-1}\neq r_1,r_2...r_{N-2}} \exp{-\beta \sum_{k=1}^N \epsilon(r_k)}] \nonumber\\&\ \ \ \ \ \times \qty[\sum_{r_N}\exp{-\beta\epsilon(r_N)}-\sum_{j=1}^{N-1}\exp{-\beta\epsilon(r_j)}]\nonumber \\
   &=\frac{1}{N}S(1)Z_F(N-1)\\
   &\ \ \ \ -\frac{1}{N!}\qty[\sum_{r_1}\sum_{r_2\neq r_1}...\sum_{r_{N-1}\neq r_1,r_2...r_{N-2}} \exp{-\beta \sum_{k=1}^N \epsilon(r_k)} \exp{-\beta \epsilon(r_j)}] \nonumber
\end{align}
In the second step, the first bracket term is just the definition of $Z_F(N-1)$ and the first term in the second bracket is $S(1)$ by the definition given for the function. These steps can be repeated with the remaining bracket to give something with the form \cite{Borr1993}
\begin{gather}
    Z_F(N)=\frac{1}{N}S(1)Z_F(N-1)-\frac{1}{N}S(2)Z_F(N-2)+\frac{1}{N}S(3)Z(N-3)-...\nonumber\\
    Z_F(N)=\frac{1}{N}\sum_{k=1}^N (-1)^{k+1} S(k)Z_F(N-k)
\end{gather}

This is a nice recursive relation that can be implemented with relative ease. The issue that arises from this is the $(-1)^{k+1}$ term. Numerically, this summation is unstable because it is constantly increasing and changing signs. Therefore, to accurately calculate the partition function, the summation must either be truncated before it fails or the numerical precision used must be increased. The latter method becomes very computationally expensive from a memory standpoint and takes a long time. Therefore, it is necessary to use a different approach. 


Instead of using this general recursive relation, it is helpful to see if there are any specific cases that can be solved. The easiest example would be that of the simple harmonic oscillator. An exact solution was found for this spectrum by Sch\"onhammer \cite{Schon1996}. This solution differs from Borrmann's because it counts the number of ways that a particular state can be created starting from the ground state. 

The shift from the ground state energy level to the first excited state is given by a change of $\Delta$, which is the difference in energy between the two levels. For the simple harmonic oscillator spectrum, $\Delta=1$. Therefore, if the particle in the ground state were to move to the second excited state, the change would be $2\Delta$. The pattern continues for higher energy levels. To generalize this idea. A system of more than one particle in the ground state configuration could be excited in such a way that some or all of the particles move to a higher state. The total sum of each of these changes would then add up to final value $M$. As an example, consider the case of $M=4$. There are five possible systems or partitions that can give this value. One would be that one particle moves up four levels, while another would be four particles moving up one level each. To account for the number of ways to get a specific value of $M$, consider that
\begin{gather}
    M=m_1+2m_2+3m_3+...+Mm_M=\sum_{\ell=1}^M \ell m_{\ell}\\
    g_m=\sum_{m_1=0}^{\infty}\sum_{m_2=0}^{\infty}...\sum_{m_M=0}^{\infty}\sum_{m_{M+1}=0}^{\infty}...\delta_{M,\sum_{\ell=1}^M \ell m_{\ell}}
\end{gather}
where $m_l$ represents the number of particles that move up $l$ levels. The value $g_m$ represents the total number of ways to construct a partition corresponding to $M$. In other words, $g_m$ is the degeneracy corresponding to a specific energy $M\Delta$.  

While it is unnecessary to include more than $M$ different summations in Equation (1.26), the infinitely extra terms help with the calculation of the partition function. To show this, consider the canonical partition function of the linear model, i.e. the simple harmonic oscillator
\begin{equation}
    Z_C(N)=\sum_{M=0}^{\infty} g_M \exp(-\beta(E_0+M\Delta)).
\end{equation}
From this equation, the value for $g_M$ can be plugged in and some simple algebra gives the exact solution as \cite{Schon1996}
\begin{align}
    Z_C(N)&=\sum_{M=0}^{\infty} \sum_{m_1=0}^{\infty}\sum_{m_2=0}^{\infty}...\sum_{m_M=0}^{\infty}... \delta_{M,\sum_{\ell=1}^M \ell m_{\ell}} \exp(-\beta(E_0+M\Delta))\nonumber\\
    &=\exp(-\beta E_0) \prod_{\ell=1}^{\infty}\qty(\sum_{m_{\ell}=0}^{\infty} \exp(-m_{\ell}\beta \ell\Delta))\nonumber\\
    &=\exp(-\beta E_0) \prod_{\ell=1}^{\infty} \frac{1}{1-\exp(-\beta \ell\Delta)}.
\end{align}
In the second line, the summation in the exponential was brought down as a product. The last line, the geometric series was solved to give the fraction. This is the exact solution for the canonical partition function of a linear spectrum. One interesting thing to note about this solution is that it can be rewritten as a recursive equation. The product is of all the terms corresponding to having $1$ through $N$ particles. Therefore, the next value in the recursion is found by multiplying the previous value by a factor as shown below
\begin{equation}
    Z_C(N)=\frac{\exp{-\beta N\Delta}}{1-\exp{-\beta N\Delta}}Z_C(N-1)
\end{equation}
with the starting value of $Z_C(1)=1$. This equation is numerically stable and can be solved either recursively or as a product. 
This exact solution provides not only a method to calculate the canonical partition function for the one dimensional simple harmonic oscillator, but also provides a result for other methodologies to check against.\\
While this solution works for the specific cases of a linear energy spectrum, a solution is still needed for other energy spectra. This has been worked out by Barghathi et al. \cite{Hatem2020}. The canonical partition function can be calculated using a summation.
\begin{equation}
    Z_N=\sum_{k=k_{min}}^{k_{max}} Z_k(S^{(1)})Z_{N-k}(S^{(2)})
\end{equation}
where $S$ denotes a spectrum that is a union of disjoint subspectra \cite{Hatem2020}. This can be written as 
\begin{gather}
    S=S^{(1)}\cup S^{(2)}\\
    S^{(1)}=\frac{S}{S^{(2)}}
    S^{(2)}=\frac{S}{S^{(1)}}
\end{gather}
Using this spectral formulation, the partition function summation can be written more specifically as 
\begin{equation}
    Z_N=\sum_{k=0}^N Z_k(\{\epsilon_1,\epsilon_2,...\epsilon_j\})Z_{N-k}^{\backslash \{\epsilon_1,\epsilon_2,...\epsilon_j\}} 
\end{equation}
where the $\backslash \{\epsilon_1,\epsilon_2,...\epsilon_j\} $ denotes the energy levels excluded from the spectrum of the particular partition function. Now let each spectrum $S^{(j)}$ consist of a single energy level. For non-interacting fermions, the partition function of a system with a single energy level is
\begin{equation}
    Z_k(\{j\})=\begin{cases} e^{-\beta \epsilon_j k} & 0\leq k \leq 1\\0 & \text{otherwise}\end{cases}
\end{equation}
By substituting this into (1.36) we see that
\begin{equation}
    Z_N=Z_N^{\backslash\{j\}}+e^{-\beta\epsilon_j}Z_{N-1}^{\backslash\{j\}}
\end{equation}
This equation can be understood by looking at each term on the right side. On the left side of the addition, the $Z_N^{\backslash\{j\}}$ term is the canonical partition function of $N$ particles with no particle in the $j$th energy level. This represents the complimentary probability, $q_j$, or the probability that energy level $\epsilon_j$ is unoccupied. The term on the right hand side has a Boltzmann probability for energy level $\epsilon_j$ times the canonical partition function of $N-1$ particles with no particle at the $j$th energy level. The Boltzmann term introduces the probability of occupying state $j$ with a particle, so the total number of particles on the right hand side of the addition is $N$, the same as the left. The state that is excluded from the partition function is reintroduced meaning that this term is the probability that energy level $\epsilon_j$ is occupied. Since the state can only be occupied or unoccupied for fermions, we can recognize that the full partition function is a sum of the unoccupied and occupied terms, or the full probability. 

To understand this better, let's consider another simple example. Consider there are two energy levels $\epsilon_1$ and $\epsilon_2$. The canonical partition functions is 
\begin{equation}
    Z(N) = \begin{cases}
    1 & N=0\\
    e^{\epsilon_1}+e^{\epsilon_2} & N=1\\
    e^{\epsilon_1+\epsilon_2} & N=2
    \end{cases}.
\end{equation}
Now using this formulation from above, break down the full spectrum into two subsectra given as 
\begin{gather}
    Z_1=\begin{cases}1&N=0\\e^{\epsilon_1}& N=1\end{cases}\\
    Z_2=\begin{cases}1&N=0\\e^{\epsilon_2}& N=1\end{cases}.
\end{gather}
Now the partition function corresponding to each $N$ can be calculated using Equation (1.35)  
\begin{gather}
    Z(0)= (1)(1)=1\\
    Z(1)= (1)(e^{-\beta\epsilon_1})+(e^{-\beta\epsilon_2})(1)=e^{-\beta\epsilon_1}+e^{-\beta\epsilon_2}\\
    Z(2)=(e^{-\beta\epsilon_1})(e^{-\beta\epsilon_2})=e^{-\beta\epsilon_1-\beta\epsilon_2}
\end{gather}
Putting these values together yields 
\begin{equation}
    Z(N) = \begin{cases}
    1 & N=0\\
    e^{\epsilon_1}+e^{\epsilon_2} & N=1\\
    e^{\epsilon_1+\epsilon_2} & N=2
    \end{cases}.
\end{equation}
which is exactly what was calculated before. So the canonical partition function can indeed be found through this method.